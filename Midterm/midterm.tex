\documentclass[11pt, a4paper]{article}
    \usepackage{amsmath, amssymb, amsthm}
    \usepackage[margin=0.1in, landscape]{geometry}
    \usepackage{graphicx}
    \usepackage{verbatim}
    \usepackage{multicol}
    \usepackage[compact]{titlesec}
    \usepackage{transparent}
    \usepackage{eso-pic}
    \usepackage[nodisplayskipstretch]{setspace} \setstretch{1.5}
    \DeclareMathOperator*{\argmax}{\arg\!\max}
    \usepackage{etoolbox}
    \preto{\verbatim}{\edef\tempstretch{\baselinestretch}\par\setstretch{1}}
    \appto{\endverbatim}{\vspace{-\tempstretch\baselineskip}\vspace{\baselineskip}}
    \titlespacing*{\section}{0pt}{-5pt}{0pt}
    \linespread{1.35}
    \setlength{\columnseprule}{0.4pt}
    \makeatletter
    \preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
    \makeatother

\begin{document}

    \begin{multicols*}{1}
        \section*{Agents}
            \textbf{Rational:} Maximally achieving goals (actions that maximize utility function) \\
            \textbf{Reflex Based:} Chooses action based on current percept (no future consideration)\\
            \textbf{Goal Based:} Chooses action based on consequences (model of how the world reacts)\\
            \textbf{Utility Based:} Goal based with trading off of multiple goals and uses probabilities
        \section*{Search}
            \textbf{Def:} Possible states, Successor function $f(n) \to$ ($n'$, action, cost), start and goal state\\
            \textbf{Complete:} Guaranteed to find a solution if one exists\\
            \textbf{Optimal:} Guaranteed to find the least cost path\\
            \textbf{Properties:} n$ =$ number of states, b$ =$ maximum branching factor, $C^*$$ =$ optimal cost, d$ =$ depth of shallowest solution, m$ =$ max depth, $\epsilon =$ min cost of all actions\\
            \textbf{Conformant Planning:} Set of actions that always work (sterilizing surgical gear)
        \section*{Blind Search}
            \textbf{DFS:} Fringe uses a Stack, complete iff finite, not optimal, time: O(b$^{m}$), space: O(bm)\\
            \textbf{BFS:} Fringe uses a Queue, complete, optimal (constant), time and space: O(b$^{d}$)\\
            \textbf{IDDFS:} Fringe uses a Stack, complete, optimal (constant), time: O(b$^{d}$), space: O(bm)
        \section*{Heuristic Search}
            \textbf{Heuristic $h(n)=$} An estimate of how close a state is to a goal\\
            \textbf{Admissible:} Always an underestimate to the true lowest cost\\
            \textbf{Consistent:} Always $h(n) <= h(n') + stepCost(n')$ where n' is a neighbor of n\\
            \textbf{Best First:} Fringe uses a PriorityQueue with cost fuction $f(n)$ for each node\\
            \textbf{Uniform Cost:} Best First with $f(n)=$ sum of edge costs from  start to n (explores increasing contours), complete, optimal, time and space: O($b^{\frac{C^*}{\epsilon}}$)\\
            \textbf{Greedy:} Best First with $f(n)= h(n)$ (suboptimal goal is common)\\
            \textbf{A$^*$:} Best First with $f(n)= g(n) + h(n)$ with $g(n)=$ sum of costs from start to n\\
            \textbf{IDA$^*$:} Depth bound is now $F_{limit} = h(start)$, prune if $f(n) > F_{limit}$,\\ $F_{limit}' = min(pruned\ nodes)$, uses space of DFS, time depends on \# of unique F values\\
            \textbf{Beam:} Best First with $|Fringe| = K$, not complete, time: O($b^d$), space: O($b + K$)\\
            \textbf{Hill Climbing:} Always choose best child (Beam Search with K = 1)\\
            \textbf{Tabu:} Keep fixed length queue of states to not visit again (use with hill climbing)\\
        \section*{Stochastic Search}
            \textbf{Hill Climbing++ Restarts:} Generate random state when plateaued\\
            \textbf{Hill Climbing++ Walk:} With prob $p$ move to the neighbor with largest value, with $(1 - p)$ move to a random neighbor\\
            \textbf{Hill Climbing++ (Both):} Greedy move, random walk, or random restart\\
            \textbf{Simulated Annealing:} Pick a random neighbor and calculate the change in `energy' or objective function $\delta$, if it is positive then move to that state. Otherwise, move to this state with probability $e^\frac{\delta}{T}$ where T is decreased as the algorithm runs longer. High T $\to$ probability of bad move is higher and vice versa\\
            \textbf{Genetic:} Start with a population of random states, use an evaluation (fitness) function, produce next generation using random selection / crossover / random mutation\\
            \textbf{Gradient Descent:} Move in the direction of the gradient at each step
        \section*{Constraint Satisfaction Problems (CSPs)}
            \textbf{Def:} Goal test is a set of constraints over the state's variables $x_i$ $\in D_i\ or\ D$\\
            \textbf{Constraint Graphs:} Nodes are variables, (multi)edges show constraints\\
            \textbf{As a Search Problem:} States defined by the values assigned so far, initially empty, Successor function assigns a value to an unassigned variable, and the Goal test checks to see if the current assignment is complete and satisfactory\\
            \textbf{Improvements:} Fix ordering with variable assignments, check constraints as you go\\
            \textbf{Forward Checking:} Cross off values that violate a constraint when added to the existing assignment (Immediate neighbors and fail if the set of possible values is empty)\\
            \textbf{Arc Consistency:} An arc $X \to Y$ is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint\\
            \textbf{Constraint Propagation:} If X loses a value, neighbors of X need to be rechecked\\
            \textbf{Min Remaining Values:} Choose the variable with fewest legal values in its domain\\
            \textbf{Max Degree:} Choose the variable in the most constraints with remaining variables\\
            \textbf{Least Constraining Value:} Given a variable, assign a value that rules out the fewest values in remaining variables\\
            \textbf{Rationale:} Want to enter most promising branch, but detect failure quickly. MRV and MD choose the variable most likely to cause failure while LCV ensures that an early value choice does not cause failure later
        \newpage
        \section*{Constraint Satisfaction Problems II (CSPs)}
            \textbf{Trees:} Choose a root variable, order the other variables so parents precede children. Remove Backward: For i = n : 2, apply RemoveIncosistent(Parent($x_i$), $x_i$) then Assign Forward: For i = 1 : n, assign $x_i$ consistently with Parent($x_i$). Time: O($nd^2$)\\
            \textbf{Proof:} After backward pass, all root-to-leaf arcs are consistent, no backtracking\\
            \textbf{Nearly Tree-Structured:} instantiate (in all ways) a set of variables such that the remaining constraint graph is a tree, Time: O($d^c(n - c)d^2$)\\
            \textbf{Iterative Improvement:} Randomly select conflicted variable, choose a value that violates the fewest constraints (hill climb with $h(n)=$ number of violated constraints)
        \section*{Adversarial Search}
            \textbf{Minimax:} The opitmal utility with a rational adversary, Time: O($b^m$), Space: O($bm$)\\
            \textbf{Pruning:} $\alpha$ is Max's best choise on a path to root. If the best value becomes worse than $\alpha$, no point in exploring children. Similiar for $\beta$. Time: O($b^{m / 2}$)
            \begin{verbatim}
def value(state, agent, alpha, beta):
    for each successor of state:
        if agent == 0:
            v = max(v, value(successor, agent + 1, alpha, beta))
            if v >= beta: return v
            alpha = max(alpha, v)
        else:
            v = min(v, value(successor, agent + 1, alpha, beta))
            if v <= alpha: return v
            beta = min(beta, v)
    return v
            \end{verbatim}
            \textbf{Heuristic Evaluation:} Scores non-terminals and returns utility of the state\\
            \textbf{Utility:} Function from state to real numbers that describe an agent's preferences\\
            \textbf{Expectimax:} Instead of the min value being chosen, the expected value for that state is returned: $\sum_{i} [value(successor_{i}) * P(successor_{i})]$\\
            \textbf{Preferences:} A is preferred to B: $A \succ B$, A is indifferent to B: $A \sim B$\\
            \textbf{Rationality:} Orderability: $(A \succ B) \vee (B \succ A) \vee (A \sim B)$,\\ Transitivity: $(A \succ B) \land (B \succ C) \implies (A \succ C)$,\\ Continuity: $A \succ B \succ C \implies \exists_{p} [p, A: 1 - p, C] \succ B$, \\Substitutability: $A \sim B \implies [p, A: 1 - p, C] \sim [q, B: 1 - q, C]$, \\Monotonicity: $A \succ B \implies (p \geq q \iff [p, A: 1 - p, B] \succeq [q, A: 1 - q, B]$
        \section*{Markov Decision Processes}
            \textbf{Def:} states, actions, Transition $T(s, a, s')$ or $P(s' | s, a)$, Reward $R(s, a, s')$ or $R(s)$\\
            \textbf{Policy:} $\pi^{*}: S \to A$, gives an optimal action for all states\\
            \textbf{Discounting:} Each time a level is descended multiply by another factor of $\gamma$, Sooner rewards have higher utility than later rewards and helps algorithm converge: $U([r_0, r_1, ...]) = r_0 + \gamma r_1 + \gamma^2r_2 + ...$\\
            \textbf{Bellman Equations:} $V^*(s) = \max\limits_{a} Q^*(s, a) = $ utility starting in s and then acting optimally, $Q^*(s, a) = \sum\limits_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*(s')] = $ utility starting after taking action a in s then acting optimally\\
            \textbf{Value Iteration:} $V_0(s) = 0$, given $V_k(s)$ simultaneously solve one ply of expectimax from each state: $V_{k + 1}(s) \gets \sum\limits_{s'} T(s, a, s') [R(s, a, s') + \gamma V_k^*(s')]$ with Time: $O(|S|^2|A|)$\\
            \textbf{Policy Extraction:} Given the optimal values, $V^*(s)$, $\pi^*(s) = \argmax\limits_{a} Q^*(s, a)$\\
            \textbf{Problems:} Slow, max at each state rarely changes and the policy often converges long before the values do\\
            \textbf{Asynchronous Value Iteration:} Not essential to back up all states in each iteration as long as no state is starved. Backup with a heap of states ordered by the expected change in value (prefer backing a state whose successors had most change\\
            \textbf{Policy Evaluation:} compute the utility of a state s under a fixed policy: $V^\pi(s) =$ expected total discounted rewards starting in s and following $\pi$\\
            \textbf{Policy Iteration:} Initialize $\pi(s)$ to random actions, calculate utilities of $\pi$ at each s using a nested loop, update policy using one-step look-ahead to see what's the best action I could execute, assuming I then follow $\pi(s)$\\
            \hspace*{5 mm}Initialize $\pi_i(s)$ to random actions and $i = 0$. Repeat:\\
            \hspace*{10 mm}Step 1: Policy Evaluation:\\
            \hspace*{15 mm}Initialize $k = 0$ and for each s, $V_0^\pi(s) = 0$ and repeat until $V^\pi$ converges:\\
            \hspace*{15 mm}For each s $V_{k + 1}^{\pi_i}(s) \gets \sum\limits_{s'} T(s, \pi_i(s), s') [R(s, \pi_i(s), s') + \gamma V_k^{\pi_i}(s')]$\\
            \hspace*{15 mm}$k++$\\
            \hspace*{10 mm}Step 2: Policy Improvement:\\
            \hspace*{15 mm}For each s, $\pi_{i + 1}(s) = \argmax\limits_{a} \sum\limits_{s'} T(s, a, s') [R(s, a, s') + \gamma V^{\pi_i}(s')]$\\
            \hspace*{15 mm}If $\pi_i == \pi_{i + 1}$, then it is optimal\\
            \hspace*{15 mm}Else let $i++$
    \end{multicols*}
\end{document}

